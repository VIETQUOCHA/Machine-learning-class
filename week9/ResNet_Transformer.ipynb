{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Multi-Head Self-Attention\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        self.query = layers.Dense(dim)\n",
        "        self.key = layers.Dense(dim)\n",
        "        self.value = layers.Dense(dim)\n",
        "        self.out = layers.Dense(dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        B = tf.shape(x)[0]\n",
        "        N = tf.shape(x)[1]\n",
        "        D = x.shape[-1]\n",
        "        H = self.num_heads\n",
        "        d = self.head_dim\n",
        "\n",
        "        Q = tf.reshape(self.query(x), (B, N, H, d))\n",
        "        K = tf.reshape(self.key(x), (B, N, H, d))\n",
        "        V = tf.reshape(self.value(x), (B, N, H, d))\n",
        "\n",
        "        Q = tf.transpose(Q, perm=[0, 2, 1, 3])\n",
        "        K = tf.transpose(K, perm=[0, 2, 1, 3])\n",
        "        V = tf.transpose(V, perm=[0, 2, 1, 3])\n",
        "\n",
        "        attn_score = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(d, tf.float32))\n",
        "        attn_weight = tf.nn.softmax(attn_score, axis=-1)\n",
        "        attn_out = tf.matmul(attn_weight, V)\n",
        "\n",
        "        attn_out = tf.transpose(attn_out, perm=[0, 2, 1, 3])\n",
        "        attn_out = tf.reshape(attn_out, (B, N, D))\n",
        "        return self.out(attn_out)\n",
        "\n",
        "\n",
        "# Transformer Encoder\n",
        "class CustomTransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim, num_heads=4, ff_dim=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(dim),\n",
        "        ])\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        attn_out = self.attn(x)\n",
        "        x = self.norm1(x + self.dropout(attn_out, training=training))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out, training=training))\n",
        "        return x\n",
        "\n",
        "\n",
        "# ResNet Block\n",
        "\n",
        "class ResNetBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv1D(channels, 3, padding='same')\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.conv2 = layers.Conv1D(channels, 3, padding='same')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out, training=training)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out, training=training)\n",
        "        out = out + residual\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "# ResNet + Transformer Classifier\n",
        "\n",
        "class ResNetTransformerClassifier(tf.keras.Model):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes,\n",
        "                 num_resnet_blocks=2, num_transformer_layers=2,\n",
        "                 num_heads=4, ff_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc0 = layers.Dense(hidden_dim)\n",
        "\n",
        "        self.resnet_blocks = [ResNetBlock(1) for _ in range(num_resnet_blocks)]\n",
        "\n",
        "        self.transformer_layers = [\n",
        "            CustomTransformerEncoderLayer(hidden_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_transformer_layers)\n",
        "        ]\n",
        "\n",
        "        self.fc_out = layers.Dense(num_classes, activation=None)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.fc0(x)\n",
        "        x = tf.expand_dims(x, axis=1)\n",
        "        x = tf.transpose(x, perm=[0, 2, 1])\n",
        "\n",
        "        for block in self.resnet_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        x = tf.transpose(x, perm=[0, 2, 1])\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, training=training)\n",
        "\n",
        "        x = tf.squeeze(x, axis=1)\n",
        "        logits = self.fc_out(x)\n",
        "        probs = tf.nn.softmax(logits, axis=-1)\n",
        "        return logits, probs\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Deep learning is amazing\",\n",
        "    \"Transformers are powerful\",\n",
        "    \"I hate overfitting\",\n",
        "    \"Neural networks are cool\"\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 0, 1]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "X = pad_sequences(sequences, maxlen=6)\n",
        "y = np.array(labels)\n",
        "\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "embedding_dim = 64\n",
        "inputs = tf.keras.Input(shape=(6,))\n",
        "x = layers.Embedding(num_words, embedding_dim)(inputs)\n",
        "\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "logits, probs = ResNetTransformerClassifier(\n",
        "    input_dim=embedding_dim,\n",
        "    hidden_dim=64,\n",
        "    num_classes=num_classes\n",
        ")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, probs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(X, y, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "ZkHOwiAApKyM",
        "outputId": "f3433410-4c71-4477-fd0a-8ed3d87cf201"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ res_net_transformer_classifier… │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)] │        \u001b[38;5;34m71,266\u001b[0m │\n",
              "│ (\u001b[38;5;33mResNetTransformerClassifier\u001b[0m)   │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ res_net_transformer_classifier… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)] │        <span style=\"color: #00af00; text-decoration-color: #00af00\">71,266</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResNetTransformerClassifier</span>)   │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m72,290\u001b[0m (282.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,290</span> (282.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,282\u001b[0m (282.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,282</span> (282.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8\u001b[0m (32.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> (32.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - accuracy: 0.6000 - loss: 0.6824\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8000 - loss: 0.2607\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.1839\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0664\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0506\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0378\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0203\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0224\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0039\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d5d701569c0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTpueoRErhVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}